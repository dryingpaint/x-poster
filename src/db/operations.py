"""Database operations for items, chunks, and web cache."""

import hashlib
from datetime import datetime
from typing import Any
from uuid import UUID

from src.core.config import get_config
from src.core.models import ItemKind, SearchResult, WebCache
from src.db.client import get_db_client


async def insert_item(
    kind: ItemKind,
    title: str | None,
    source_uri: str,
    content_text: str,
    meta: dict[str, Any] | None = None,
) -> UUID:
    """Insert a new item into the database."""
    db = get_db_client()
    client = db.get_client()

    data = {
        "kind": kind.value,
        "title": title,
        "source_uri": source_uri,
        "content_text": content_text,
        "meta": meta or {},
    }

    result = client.table("items").insert(data).execute()
    return UUID(result.data[0]["item_id"])


async def insert_chunks(item_id: UUID, chunks: list[dict[str, Any]]) -> list[UUID]:
    """Insert chunks for an item."""
    db = get_db_client()
    client = db.get_client()

    # Prepare chunk data
    chunk_data = []
    for chunk in chunks:
        chunk_data.append(
            {
                "item_id": str(item_id),
                "content": chunk["content"],
                "embedding": chunk.get("embedding"),
                # tsv will be generated by trigger
            }
        )

    result = client.table("item_chunks").insert(chunk_data).execute()
    return [UUID(row["chunk_id"]) for row in result.data]


async def search_internal(
    query: str, query_embedding: list[float], top_k: int = 50
) -> list[SearchResult]:
    """
    Hybrid search: combine full-text (tsvector) and vector (KNN) search.
    Returns unified list of SearchResult objects.
    """
    db = get_db_client()
    client = db.get_client()

    # Full-text search using tsvector
    fts_results = (
        client.rpc(
            "search_chunks_fts",
            {
                "search_query": query,
                "match_count": top_k,
            },
        )
        .execute()
        .data
    )

    # Vector similarity search
    vector_results = (
        client.rpc(
            "search_chunks_vector",
            {
                "query_embedding": query_embedding,
                "match_count": top_k,
            },
        )
        .execute()
        .data
    )

    # Merge results by chunk_id
    seen = set()
    merged = []

    for result in fts_results + vector_results:
        chunk_id = result["chunk_id"]
        if chunk_id in seen:
            continue
        seen.add(chunk_id)

        # Carry chunk_id in meta so downstream can fetch stored embeddings
        meta = result.get("meta", {}) or {}
        meta["chunk_id"] = str(chunk_id)

        merged.append(
            SearchResult(
                source_id=f"internal_{chunk_id}",
                content=result["content"],
                title=result.get("title"),
                source_uri=result.get("source_uri"),
                author=meta.get("author"),
                published_at=None,
                meta=meta,
                score=float(result.get("score", 0.0) or 0.0),
                source_type="internal",
            )
        )

    return merged[:top_k]


async def get_chunk_embeddings(chunk_ids: list[str]) -> list[list[float]]:
    """Fetch stored embeddings for given chunk_ids in input order.

    Any missing embeddings are returned as empty lists to preserve alignment.
    """
    if not chunk_ids:
        return []

    db = get_db_client()
    client = db.get_client()

    result = (
        client.table("item_chunks")
        .select("chunk_id,embedding")
        .in_("chunk_id", chunk_ids)
        .execute()
    )

    rows = result.data or []
    emb_by_id: dict[str, list[float]] = {}
    for row in rows:
        cid = str(row.get("chunk_id"))
        emb = row.get("embedding")
        if isinstance(emb, list):
            emb_by_id[cid] = emb

    ordered: list[list[float]] = []
    for cid in chunk_ids:
        ordered.append(emb_by_id.get(str(cid), []))

    return ordered


async def cache_web_page(
    url: str,
    domain: str,
    title: str | None,
    content: str,
    embedding: list[float] | None = None,
    published_at: datetime | None = None,
) -> str:
    """Cache a web page for reuse."""
    db = get_db_client()
    client = db.get_client()

    url_hash = hashlib.sha256(url.encode()).hexdigest()

    data = {
        "url_hash": url_hash,
        "url": url,
        "domain": domain,
        "title": title,
        "content": content,
        "embedding": embedding,
        "published_at": published_at.isoformat() if published_at else None,
    }

    client.table("web_cache").upsert(data).execute()
    return url_hash


async def get_cached_web_page(url: str) -> WebCache | None:
    """Retrieve a cached web page if available and fresh."""
    db = get_db_client()
    client = db.get_client()
    config = get_config()

    url_hash = hashlib.sha256(url.encode()).hexdigest()

    result = (
        client.table("web_cache")
        .select("*")
        .eq("url_hash", url_hash)
        .execute()
    )

    if not result.data:
        return None

    data = result.data[0]

    # Check if cache is still fresh
    fetched_at = datetime.fromisoformat(data["fetched_at"])
    age_seconds = (datetime.now() - fetched_at).total_seconds()

    if age_seconds > config.web_cache_ttl:
        return None

    return WebCache(**data)

